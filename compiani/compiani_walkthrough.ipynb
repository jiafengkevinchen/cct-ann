{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import argparse\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from statsmodels.tools import add_constant\n",
    "from numpy.polynomial import chebyshev\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50) \n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from run_model import generate_parser, main\n",
    "from pipeline.pipeline import (\n",
    "    compute_inverse_design,\n",
    "    loss_fn,\n",
    "    loss_fn_qiv,\n",
    "    null_regularizer,\n",
    "    stopping_criterion,\n",
    "    train_loop,\n",
    "    train_step,\n",
    "    transform_endogenous_wrapper,\n",
    "    weight_fn,\n",
    "    l1_regularizer\n",
    ")\n",
    "\n",
    "from architecture.architectures import (\n",
    "    Nonparametric,\n",
    "    PartiallyAdditive,\n",
    "    PartiallyAdditiveWithSpline,\n",
    "    PartiallyLinear,\n",
    ")\n",
    "\n",
    "from pipeline.callbacks import callback, log_callback, tensorboard_callback, writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q_own             float64\n",
       "q_other           float64\n",
       "s_own             float64\n",
       "s_other           float64\n",
       "p_own             float64\n",
       "p_other           float64\n",
       "p_out             float64\n",
       "p_out_avg         float64\n",
       "x_own             float64\n",
       "x_other           float64\n",
       "x_month           float64\n",
       "x_usda_lettuce    float64\n",
       "x_outf            float64\n",
       "income            float64\n",
       "z_own             float64\n",
       "z_other           float64\n",
       "z_out             float64\n",
       "p_change            int64\n",
       "spot_own          float64\n",
       "spot_other        float64\n",
       "p_own_nonn        float64\n",
       "p_other_nonn      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organic_strawb.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_definitions = {\n",
    "    \"s_own\": \"market share (for a given store/week combination) of product 1 (=non-organic strawberries)\",\n",
    "    \"s_other\": \"market share of product 2  (=organic strawberries)\",\n",
    "    \"p_own\": \"price per pound of product 1\",\n",
    "    \"p_other\": \"price per pound of product 2\",\n",
    "    \"p_out_avg\": \"price per pound of outside option (=other fruit)\",\n",
    "    \"x_usda_lettuce\": \"proxy for taste for organic products at store\",  # x_org^1\n",
    "    \"x_outf\": \"proxy for richness of the outside option\",  # x_str^1 ?\n",
    "    \"income\": \"income at zipcode level\",  # x^2\n",
    "    \"spot_own\": \"spot price (a measure of marginal cost) for product 1\",\n",
    "    \"spot_other\": \"spot price for product 2\",\n",
    "    \"z_own\": \"Hausman IV (price of same product in neighboring markets) for product 1\",\n",
    "    \"z_other\": \"Hausman IV (price of same product in neighboring markets) for product 2\",\n",
    "    \"z_out\": \"Hausman IV (price of same product in neighboring markets) for outside option (other fruit)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Basic Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "strawb = pd.read_csv(\"data_1.csv\")\n",
    "organic_strawb = pd.read_csv(\"data_2.csv\")\n",
    "\n",
    "\n",
    "# drop redundant columns and columns with 0 variation\n",
    "string_cols =  list(strawb.columns[:5])+list(strawb.columns[-9:])\n",
    "redund_strawb = string_cols + ['usda'] + ['x_usda_lettuce_2'] + ['x_outf_2']\n",
    "redund_org_strawb = string_cols+ ['usda'] + ['x_usda_lettuce_2'] + ['x_outf_2']\n",
    "strawb = strawb.drop(columns = redund_strawb)\n",
    "organic_strawb = organic_strawb.drop(columns = redund_org_strawb)\n",
    "\n",
    "# transform variables in logs so that by finding \n",
    "# the average derivative we obtain our functional\n",
    "# of interest - elasticity\n",
    "strawb['q_own'] = np.log(strawb['q_own'])\n",
    "strawb['q_other'] = np.log(strawb['q_other'])\n",
    "strawb['p_own'] = np.log(strawb['p_own'])\n",
    "strawb['p_other'] = np.log(strawb['p_other'])\n",
    "organic_strawb['q_own'] = np.log(organic_strawb['q_own'])\n",
    "organic_strawb['q_other'] = np.log(organic_strawb['q_other'])\n",
    "organic_strawb['p_own'] = np.log(organic_strawb['p_own'])\n",
    "organic_strawb['p_other'] = np.log(organic_strawb['p_other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be estimating the following equation using the NPIV model:\n",
    "$log(q_{jt}) = h(log(p_{jt}),log(p_{-jt}), x_{jt},m_{t})$ \\\n",
    "Our instruments will be Hausman type IV that act as a proxy of marginal cost shifters. This is not the model \\\n",
    "used in Compiani 2021, but a simplified version since the NPIV approach with NN as a first stage estimator \\\n",
    "performed poorly with his estimating equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "## different instrument transforms \n",
    "\n",
    "# no interactions\n",
    "def random_fourier_basis(x, scale=5, d=60, seed=1):\n",
    "    n, d_in = x.shape\n",
    "    rng = np.random.RandomState(seed)\n",
    "    w = rng.randn(d_in, d) * scale\n",
    "    b = rng.rand(1, d) * 2 * np.pi\n",
    "    return add_constant(np.cos(x @ w + b))\n",
    "\n",
    "# with interactions\n",
    "def chebyshev_expansion(var, max_degree=3, intercept=False):\n",
    "    expansion = []\n",
    "    for i in range(1 - int(intercept), max_degree + 1):\n",
    "        c = np.zeros(max_degree + 1)\n",
    "        c[i] = 1\n",
    "        expansion.append(chebyshev.chebval(2 * var - 1, c))\n",
    "    return np.array(expansion).T\n",
    "\n",
    "## data preparation \n",
    "\n",
    "def wrangle_data_quantity_regr(organic=False, chebyshev=False):\n",
    "    \n",
    "    # specify what column is the outcome variable,\n",
    "    # what columns are endogenous (they enter the unknown NPIV h() function)\n",
    "    # and what columns are instruments\n",
    "    y = \"q_own\"\n",
    "    endo_cols = [\"p_own\", \"p_other\", 'x_usda_lettuce','x_outf', \"income\"] # order of variables is important\n",
    "    instrument_cols = [\n",
    "        'x_usda_lettuce',\n",
    "        'x_outf',\n",
    "        \"income\",\n",
    "        \"spot_own\",\n",
    "        \"spot_other\",\n",
    "        \"z_own\",\n",
    "        \"z_other\",\n",
    "        \"z_out\",\n",
    "    ]\n",
    "\n",
    "    data = organic_strawb if organic else strawb\n",
    "\n",
    "    response = data[y].values.copy()[:, None]\n",
    "    endogenous = data[endo_cols].values.copy()\n",
    "    instrument = data[instrument_cols].values.copy()\n",
    "\n",
    "    # form an instrument basis that is used to project\n",
    "    # on an instrument space and obtain conditional expectation\n",
    "    if chebyshev:\n",
    "        transformed_instrument = add_constant(\n",
    "            np.hstack(\n",
    "                [chebyshev_expansion(var, intercept=False) for var in instrument.T]\n",
    "                + [\n",
    "                    (i * j)[:, None]\n",
    "                    for i in instrument.T\n",
    "                    for j in instrument.T\n",
    "                    if (i != j).any()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        transformed_instrument = random_fourier_basis(instrument)\n",
    "\n",
    "    # organises our data objects as a dictionary\n",
    "    # that will be an input in our model\n",
    "    npvec = {\n",
    "        \"response\": response,\n",
    "        \"endogenous\": endogenous,\n",
    "        \"instrument\": instrument,\n",
    "        \"transformed_instrument\": transformed_instrument,\n",
    "        \"inverse_design_instrument\": np.linalg.pinv(\n",
    "            transformed_instrument.T\n",
    "            @ transformed_instrument\n",
    "            / len(transformed_instrument),\n",
    "            rcond=1e-5,\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    # prints the rank of the design matrix\n",
    "    print(np.linalg.matrix_rank(npvec[\"inverse_design_instrument\"]), npvec[\"inverse_design_instrument\"].shape)\n",
    "\n",
    "    # transforms data dictionary into pytorch format\n",
    "    torchvec = {k: torch.as_tensor(v).float() for k, v in npvec.items()}\n",
    "    return npvec, torchvec\n",
    "\n",
    "\n",
    "## derivatives & elasticities\n",
    "\n",
    "# uses pytorch to find a partial derivative of the model in a certain variable\n",
    "def get_deriv(model, torchvec, which=\"p_own\"):\n",
    "    lst = \"p_own p_other x_usda_lettuce x_outf income\".split()\n",
    "    index = lst.index(which)\n",
    "    return model.get_derivatives(torchvec['endogenous'], index=index).numpy().flatten()\n",
    "\n",
    "# # this one is for derivatives at a particular point - works poorly\n",
    "# def get_deriv_elast(model, torchvec, which=\"s_own\"):\n",
    "#     lst = \"s_own s_other p_own p_other\".split()\n",
    "#     index = lst.index(which)\n",
    "#     return model.get_derivatives_for_elast(torchvec, index=index)\n",
    "\n",
    "\n",
    "\n",
    "# applies get_deriv to find own and cross price elasticities \n",
    "def get_elast_quantity_regr(own_model, own_torchvec):\n",
    "    top = get_deriv(own_model, own_torchvec, \"p_own\")\n",
    "    p_own = own_torchvec[\"endogenous\"][:, 0].numpy().flatten()\n",
    "    \n",
    "    return top, (p_own, top)\n",
    "\n",
    "def get_cross_elast_quantity_regr(own_model, own_torchvec):\n",
    "    top = get_deriv(own_model, own_torchvec, \"p_other\")\n",
    "    p_other = own_torchvec[\"endogenous\"][:, 1].numpy().flatten()\n",
    "    \n",
    "    return top, (p_other,  top)\n",
    "\n",
    "def fit_model(torchvec, model_kwargs, optimizer_kwargs, name, max_iter=3000, with_weights = False):\n",
    "    \n",
    "    # load the specific model class that is suitable\n",
    "    # for this problem. It could be one of the following:\n",
    "    # Nonparametric, PartiallyAdditive,\n",
    "    # PartiallyAdditiveWithSpline, PartiallyLinear.\n",
    "    # Here I use the most general Nonparametric class\n",
    "    \n",
    "    model = Nonparametric(\n",
    "        input_dim=torchvec[\"endogenous\"].shape[1], **model_kwargs\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), **optimizer_kwargs)\n",
    "\n",
    "    #training\n",
    "    \n",
    "    # basic model\n",
    "    for i in tqdm(range(max_iter)):\n",
    "        outcome = train_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            response=torchvec[\"response\"],\n",
    "            endogenous=torchvec[\"endogenous\"],\n",
    "            transformed_instrument=torchvec[\"transformed_instrument\"],\n",
    "            inverse_design_instrument=torchvec[\"inverse_design_instrument\"]\n",
    "        )\n",
    "        \n",
    "        # stores intermediate results of training\n",
    "        # could be useful to track how the model is learning\n",
    "        if i % 20 == 0:\n",
    "            result_dict = {\n",
    "                item: get_deriv(model, torchvec, which=item).mean()\n",
    "                for item in \"p_own p_other x_usda_lettuce x_outf income\".split()\n",
    "            }\n",
    "            result_dict.update(\n",
    "                {\"loss\": outcome[\"loss\"], \"grad_norm\": outcome[\"grad_norm\"]}\n",
    "            )\n",
    "\n",
    "            tensorboard_callback(\n",
    "                i, cache_df=pd.Series(result_dict).to_frame().T, name=name,\n",
    "            )\n",
    "            \n",
    "    # efficiency weighted model        \n",
    "    if with_weights:\n",
    "            \n",
    "        weights = weight_fn(\n",
    "            prediction=model(torchvec[\"endogenous\"]),\n",
    "            truth=torchvec[\"response\"],\n",
    "            basis=torchvec[\"instrument\"],\n",
    "            n_neighbors=5,\n",
    "        )\n",
    "        torchvec[\"weights\"] = weights\n",
    "        \n",
    "        model_w_weights = Nonparametric(\n",
    "        input_dim=torchvec[\"endogenous\"].shape[1], **model_kwargs)\n",
    "        optimizer = torch.optim.Adam(model_w_weights.parameters(), **optimizer_kwargs)\n",
    "        \n",
    "        for i in tqdm(range(max_iter)):\n",
    "            outcome = train_step(\n",
    "                model_w_weights,\n",
    "                optimizer,\n",
    "                response=torchvec[\"response\"],\n",
    "                endogenous=torchvec[\"endogenous\"],\n",
    "                transformed_instrument=torchvec[\"transformed_instrument\"],\n",
    "                inverse_design_instrument=torchvec[\"inverse_design_instrument\"],\n",
    "                weights=weights\n",
    "            )\n",
    "            \n",
    "        # stores intermediate results of training\n",
    "        # could be useful to track how the model is learning\n",
    "        if i % 20 == 0:\n",
    "            result_dict = {\n",
    "                item: get_deriv(model_w_weights, torchvec, which=item).mean()\n",
    "                for item in \"p_own p_other x_usda_lettuce x_outf income\".split()\n",
    "            }\n",
    "            result_dict.update(\n",
    "                {\"loss\": outcome[\"loss\"], \"grad_norm\": outcome[\"grad_norm\"]}\n",
    "            )\n",
    "\n",
    "            tensorboard_callback(\n",
    "                i, cache_df=pd.Series(result_dict).to_frame().T, name=name,\n",
    "            )\n",
    "        return model, model_w_weights, optimizer\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        return model, optimizer\n",
    "\n",
    "# Needs more comments!!    \n",
    "# This function computes standard errors \n",
    "# See the paper and Github Repo for more details\n",
    "def compute_se(\n",
    "    torchvec,\n",
    "    model,\n",
    "    weights=None,\n",
    "    inefficient_derivative=None,\n",
    "    inefficient_prediction=None,\n",
    "    order=1,\n",
    "    weighting=False,\n",
    "):\n",
    "    try:\n",
    "        se_nonpar = None\n",
    "        dim_x_tilde = torchvec[\"endogenous\"].shape[1] - 3\n",
    "        if hasattr(model, \"get_standard_error_nonparametric\"):\n",
    "            (\n",
    "                tf_endogenous,\n",
    "                tf_endogenous_gradient,\n",
    "                transformed_instrument,\n",
    "                inverse_design,\n",
    "            ) = transform_endogenous_wrapper(\n",
    "                torchvec[\"endogenous\"],\n",
    "                torchvec[\"instrument\"],\n",
    "                torchvec[\"transformed_instrument\"],\n",
    "                True,\n",
    "                dim_x_tilde,\n",
    "                order,\n",
    "                interact_x=True,\n",
    "            )\n",
    "\n",
    "            inv_variance = weight_fn(\n",
    "                prediction=model(torchvec[\"endogenous\"]),\n",
    "                truth=torchvec[\"response\"],\n",
    "                basis=transformed_instrument,\n",
    "                inverse_design=inverse_design,\n",
    "            )\n",
    "\n",
    "            if weighting:\n",
    "                filtered, Gamma = model.forward_filter_residuals(\n",
    "                    endogenous=torchvec[\"endogenous\"],\n",
    "                    response=torchvec[\"response\"],\n",
    "                    inefficient_derivative=inefficient_derivative,\n",
    "                    inefficient_prediction=inefficient_prediction,\n",
    "                    weights=weights,\n",
    "                    basis=transformed_instrument,\n",
    "                    inverse_design=inverse_design,\n",
    "                )\n",
    "\n",
    "                se_nonpar = model.get_standard_error_nonparametric(\n",
    "                    filtered,\n",
    "                    Gamma,\n",
    "                    tf_endogenous,\n",
    "                    tf_endogenous_gradient,\n",
    "                    transformed_instrument,\n",
    "                    inv_variance,\n",
    "                    inverse_design,\n",
    "                )\n",
    "            else:\n",
    "                se_nonpar = model.get_standard_error_nonparametric(\n",
    "                    model.get_derivatives(torchvec[\"endogenous\"]),\n",
    "                    0,\n",
    "                    tf_endogenous,\n",
    "                    tf_endogenous_gradient,\n",
    "                    transformed_instrument,\n",
    "                    1,\n",
    "                    inverse_design,\n",
    "                    weighting=False,\n",
    "                    residuals=torchvec[\"response\"] - model(torchvec[\"endogenous\"]),\n",
    "                )\n",
    "\n",
    "        (\n",
    "            tf_endogenous,\n",
    "            tf_endogenous_gradient,\n",
    "            transformed_instrument,\n",
    "            inverse_design,\n",
    "        ) = transform_endogenous_wrapper(\n",
    "            torchvec[\"endogenous\"],\n",
    "            torchvec[\"instrument\"],\n",
    "            torchvec[\"transformed_instrument\"],\n",
    "            False,\n",
    "            dim_x_tilde,\n",
    "            order,\n",
    "        )\n",
    "\n",
    "        inverse_variance = weight_fn(\n",
    "            prediction=model(torchvec[\"endogenous\"]),\n",
    "            truth=torchvec[\"response\"],\n",
    "            basis=transformed_instrument,\n",
    "            inverse_design=inverse_design,\n",
    "        )\n",
    "        se = model.get_standard_error(\n",
    "            endogenous_of_interest=torchvec[\"endogenous\"][:, [0]],\n",
    "            transformed_endogenous=tf_endogenous,\n",
    "            transformed_instrument=transformed_instrument,\n",
    "            inverse_variance=inverse_variance,\n",
    "            inverse_design_instrument=inverse_design,\n",
    "        )\n",
    "\n",
    "        return se, se_nonpar\n",
    "    except RuntimeError:\n",
    "        return np.nan, np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the data for training and specify the instrument basis. As an output we get torch dictionary with data that will be used in training. Also, the rank of the design matrix is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 (61, 61)\n",
      "61 (61, 61)\n"
     ]
    }
   ],
   "source": [
    "npvec_strawb, torchvec_strawb = wrangle_data_quantity_regr(organic=False, chebyshev=False)\n",
    "npvec_organic, torchvec_organic = wrangle_data_quantity_regr(organic=True, chebyshev=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is necessary to specify the NN architecture. Here the user has multiple options that affect the performance. The most obvious ones include the number of layers (depth) and the width of each layer. Also, activation function can matter in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(depth=1, width=20, hidden_activation=torch.nn.ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train models for organic and non-organic strawberry. Results can depend significantly on the learning rate, and the number of iterations. There is no clear way theoretically on how to choose those hyperparameters. Important note is that there are various options for model training within the model class. For instance, instead of using identity weighting one can efficiently weight the data using some initial consistent estimator. Also, instead of using initial moment condition one can work with orthogonalised moments to improve efficiency. I encourage you to check the paper and **pipeline** and **architecture** folders to learn more about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b11b763dab34f6a863296f573911989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24139e3a6614425b9d5a32ee396fd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train models on our data\n",
    "model_strawb, opt_strawb = fit_model(\n",
    "    torchvec_strawb,\n",
    "    model_kwargs,\n",
    "    dict(lr=0.001, weight_decay=1e-6),\n",
    "    max_iter=2000,\n",
    "    name=f\"strawb.{datetime.datetime.now()}\",\n",
    "    with_weights = False\n",
    ")\n",
    "model_organic, opt_organic = fit_model(\n",
    "    torchvec_organic,\n",
    "    model_kwargs,\n",
    "    dict(lr=0.001, weight_decay=1e-6),\n",
    "    max_iter=2000,\n",
    "    name=f\"organic.{datetime.datetime.now()}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline package provides a function *compute_se* to obtain standard errors for the estimated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes standard errors\n",
    "# at this point it requires variable of interest to be the first \n",
    "# in torchvec \n",
    "se, se_nonparam = compute_se(torchvec_strawb, model_strawb)\n",
    "se_org, se_nonparam_org = compute_se(torchvec_organic, model_organic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "elast, ingredients = get_elast_quantity_regr(\n",
    "    model_strawb, torchvec_strawb)\n",
    "cross_elast, ingredients_cross = get_cross_elast_quantity_regr(model_strawb, torchvec_strawb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "elast_org, ingredients_org = get_elast_quantity_regr(\n",
    "    model_organic, torchvec_organic)\n",
    "cross_elast_org, ingredients_cross_org = get_cross_elast_quantity_regr(\n",
    "    model_organic, torchvec_organic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-organic elasticities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elast: -2.68\n",
      "Cross Elast: -1.68\n"
     ]
    }
   ],
   "source": [
    "print(f\"Elast: {np.format_float_positional(np.mean(elast),2)}\")\n",
    "print(f\"Cross Elast: {np.format_float_positional(np.mean(cross_elast), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organic elasticites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elast: -2.14\n",
      "Cross Elast: 0.19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Elast: {np.format_float_positional(np.mean(elast_org),2)}\")\n",
    "print(f\"Cross Elast: {np.format_float_positional(np.mean(cross_elast_org),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that organic strawberries are less price elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(elast).describe(percentiles=np.linspace(0, 1, 51))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
